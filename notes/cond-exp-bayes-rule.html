<!DOCTYPE html>
<html lang="en">
  <!--Head-->
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Chenjia Lin">
    <title> Revisiting Bayes' theorem </title>
    <link rel="stylesheet" href="../styles/style.css">
  </head>

  <!--MathJax-->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <!--Header-->
  <header>
    <nav class="navbar">
      <ul>
        <li class="navli"><a href="../index.html" target="_self">About Me</a></li>
        <li class="navli"><a href="../teaching.html" target="_self">Teaching</a></li>
        <li class="navli"><a href="../articles.html" target="_self">Notes</a></li>
      </ul>
    </nav>
  </header>

  <body>
    <h1>Bayes rule (done rigorously)</h1>
    <p style="line-height: 1.8;">
      In a first course on probability, we learn about Bayes' rule: for any 
      two events $A$ and $B$, we have 
      \begin{align*}
        \mathbb{P}(A\cap B) = \mathbb{P}(A\mid B)\mathbb{P}(B).
      \end{align*}
      In this note, we will revisit this from the machinery given by conditional
      expectation.
    </p>
    <p>
      The definition of conditional expectation is given as follows.
    </p>
    <details>
      <summary>Definition of conditional expectation</summary>
    </details>

    <p style="line-height: 1.8;">
      <b>Definition.</b> Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and 
      $\mathcal{G}$ a $\sigma$-subalgebra of $\mathcal{F}$. The <em>conditional
        expectation</em> of a random variable $X:\Omega\to \mathbb{R}$ is the 
        random variable $E[X\mid \mathcal{G}]$ such that
        <ol>
          <li>$E[X\mid \mathcal{G}]$ is $\mathcal{G}$-measureable,</li>
          <li>$\mathbb{E}\big[\mathbf{1}_A E[X\mid \mathcal{G}]\big] = 
            \mathbb{E}\big[\mathbf{1}_A X\big]$ for any $A\in \mathcal{G}$, i.e. 
            \begin{align*} 
              \int_A E[X\mid \mathcal{G}] d\mathbb{P} = \int_A Xd\mathbb{P}.
            \end{align*}
          </li>
        </ol>
    </p>
    <p style="line-height: 1.8;">
      Let $\mathcal{G}_B$ be the $\sigma$-algebra generated by $B$, or 
      $\mathbf{1}_B$. By the tower law, 
      \begin{align*}
        \mathbb{E}\big[\mathbf{1}_B E[\mathbf{1}_A\mid \mathbf{1}_B]\big] = 
        \mathbb{E}\big[\mathbf{1}_A\mathbf{1}_B\big] = \mathbb{P}(A\cap B).
      \end{align*}
      An important fact that we'll need here is that $E[\mathbf{1}_A\mid \mathbf{1}_B]$
      is constant on $A$, i.e. $E[\mathbf{1}_A\mid \mathbf{1}_B](\omega_1) = 
      E[\mathbf{1}_A\mid \mathbf{1}_B](\omega_2)$ for any $\omega_1,\omega_2\in B$.
    </p>
    <p style="line-height: 1.8;">
      This follows from $E[\mathbf{1}_A\mid \mathbf{1}_B]$ being 
      $\mathcal{G}_B$-measureable. Notice that 
      $\mathcal{G}_B = \{\varnothing, B, B^c, \Omega\}$. Any $\mathcal{G}_B$-measureable
      random variable $Y$ satisfies $Y^{-1}(\{t\})\in \{\varnothing, B, B^c, \Omega\}$.
      If $\omega_0\in B$, then it follows that $Y^{-1}(\{Y(\omega)\}) $ must be
      $B$. As a result, we must have $E[\mathbf{1}_A\mid \mathbf{1}_B] = 
      u\mathbf{1}_B + v\mathbf{1}_{B^c}$. Plugging back into the original
      expression shows that $u = \mathbb{P}(A\cap B) / \mathbb{P}(B)$.
    </p>

    
  </body>
